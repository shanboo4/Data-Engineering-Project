{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Data Delta Lake Project\n",
        "\n",
        "This project demonstrates how to work with Delta Lake using Databricks Community Edition. \n",
        "We perform data ingestion, Delta table creation, and DML operations like **INSERT, UPDATE, DELETE, MERGE**, \n",
        "along with Delta-specific features such as **time travel** and **history tracking**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n",
        "We start by importing the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize Spark Session\n",
        "We create a Spark session with Delta support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SalesDataDeltaLakeProject\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Sample Data\n",
        "We create a small DataFrame with order data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [\n",
        "    (1, '2025-09-01', 'C001', 'Mobile', 2, 500),\n",
        "    (2, '2025-09-02', 'C002', 'Tablet', 1, 300),\n",
        "    (3, '2025-09-03', 'C003', 'Headphones', 5, 50)\n",
        "]\n",
        "\n",
        "columns = [\"order_id\", \"order_date\", \"customer_id\", \"product\", \"quantity\", \"price\"]\n",
        "df_spark = spark.createDataFrame(data, columns)\n",
        "df_spark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Write Data as Delta Table\n",
        "We save the DataFrame as a Delta table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_data_delta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run Queries\n",
        "We can query the Delta table using SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM sales_data_delta\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Perform Updates\n",
        "We update product prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "UPDATE sales_data_delta\n",
        "SET price = price + 50\n",
        "WHERE product = 'Mobile'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Perform Deletes\n",
        "We delete rows for specific products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "DELETE FROM sales_data_delta\n",
        "WHERE product = 'Laptop'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Merge Operation (Upsert)\n",
        "We demonstrate how to perform MERGE (UPSERT)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_table = DeltaTable.forName(spark, \"sales_data_delta\")\n",
        "\n",
        "new_data = [(2, '2025-09-02', 'C002', 'Tablet', 2, 350),\n",
        "            (4, '2025-09-05', 'C004', 'Charger', 3, 20)]\n",
        "\n",
        "df_new = spark.createDataFrame(new_data, columns)\n",
        "\n",
        "delta_table.alias(\"t\").merge(\n",
        "    df_new.alias(\"s\"),\n",
        "    \"t.order_id = s.order_id\"\n",
        ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Time Travel\n",
        "We can query older versions of the Delta table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM sales_data_delta VERSION AS OF 0\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: History\n",
        "We can check the full history of the Delta table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"DESCRIBE HISTORY sales_data_delta\").show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
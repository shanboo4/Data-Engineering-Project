# Sales Data - Delta Lake Project

This project demonstrates how to use **Databricks with PySpark & Delta Lake** for a mini end-to-end data engineering workflow.

## ğŸš€ Features
- Load CSV into Spark DataFrame
- Convert to Delta Lake format
- Perform Aggregations & Filters
- Delta Lake DML Operations:
  - UPDATE
  - DELETE
  - MERGE (Upserts)
- Time Travel (query historical versions)
- VACUUM & OPTIMIZE

## ğŸ› ï¸ Requirements
- Databricks Community Edition (Free)
- Spark (comes pre-installed with Databricks)
- Delta Lake (enabled by default in Databricks)

## ğŸ“‚ Files
- `sales_data_delta_project.ipynb` â†’ Main notebook with step-by-step code & markdowns
- `sample_data.csv` â†’ Example dataset (upload via Databricks â†’ Workspace â†’ Upload File)

## â–¶ï¸ How to Run
1. Clone this repo:
   ```bash
   git clone https://github.com/<your-username>/sales-data-delta-lake.git
   ```
2. Import the notebook into Databricks:
   - Go to **Workspace â†’ Import â†’ Upload**
   - Select `sales_data_delta_project.ipynb`
3. Upload `sample_data.csv` into your Databricks workspace
4. Run the notebook step by step

## ğŸ“Š Sample Queries
- Top products by revenue
- Orders by customer
- Historical snapshots using Delta **Time Travel**

## ğŸ“¸ Screenshots
*(Optional: you can add screenshots of your Delta table queries here)*

---

ğŸ’¡ This project is part of my **Data Engineering portfolio** â€” showcasing Spark & Delta Lake skills in Databricks.
